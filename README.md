# Kubernetes Cluster Automation with Ansible

DÃ©ploiement automatisÃ© d'un cluster Kubernetes (1 Master + Workers) avec Ansible.

## ğŸš€ Ce qui est installÃ©

- **Docker** avec configuration optimisÃ©e (cgroupdriver systemd)
- **cri-dockerd** v0.3.15 (tÃ©lÃ©chargÃ© depuis GitHub)
- **CNI plugins** v1.5.0
- **kubeadm, kubelet, kubectl** v1.29
- **Calico** network CNI
- **Local Path Storage Provisioner** (storage class par dÃ©faut)

## ğŸ“‹ PrÃ©requis

- **Machines Ubuntu 22.04** (ou 20.04)
- **AccÃ¨s SSH** Ã  tous les nÅ“uds
- **Ansible** installÃ© sur la machine de contrÃ´le (version 2.9+)
- **ClÃ© SSH** configurÃ©e (`~/.ssh/id_ed25519`)

## ğŸ”§ Configuration

### 1. Configurer les adresses IP statiques sur vos VMs

Configurez les IPs statiques avec Netplan sur chaque VM (fichier `/etc/netplan/00-installer-config.yaml`).

### 2. Configurer SSH

```bash
# GÃ©nÃ©rer une clÃ© SSH
ssh-keygen -t ed25519 -C "k8s-cluster"

# Copier sur toutes les VMs
ssh-copy-id -i ~/.ssh/id_ed25519.pub ubuntu@192.168.1.10
ssh-copy-id -i ~/.ssh/id_ed25519.pub ubuntu@192.168.1.20
ssh-copy-id -i ~/.ssh/id_ed25519.pub ubuntu@192.168.1.30
```

### 3. Configurer sudo sans mot de passe

Sur chaque VM :
```bash
sudo visudo
# Ajouter : ubuntu ALL=(ALL) NOPASSWD:ALL
```

### 4. Modifier l'inventaire

Ã‰ditez `inventory.ini` avec vos adresses IP :

```ini
[masters]
master1 ansible_host=192.168.1.10

[workers]
worker1 ansible_host=192.168.1.20
worker2 ansible_host=192.168.1.30

[all:vars]
ansible_user=ubuntu
ansible_become=true
ansible_ssh_private_key_file=~/.ssh/id_ed25519
```

### 5. Personnaliser les variables (optionnel)

Ã‰ditez `group_vars/all.yml` pour modifier :
- Version de Kubernetes
- RÃ©seau des pods
- URL Calico

## ğŸš€ DÃ©ploiement

```bash
# Tester la connectivitÃ©
ansible all -i inventory.ini -m ping

# Tester sudo
ansible all -i inventory.ini -m shell -a "sudo whoami"

# VÃ©rifier la syntaxe
ansible-playbook site.yml --syntax-check

# DÃ©ployer le cluster (mode dry-run)
ansible-playbook -i inventory.ini site.yml --check

# DÃ©ployer le cluster
ansible-playbook -i inventory.ini site.yml
```

## âœ… VÃ©rification

```bash
# Se connecter au master
ssh ubuntu@192.168.1.10

# VÃ©rifier les nÅ“uds
kubectl get nodes

# Doit afficher :
# NAME      STATUS   ROLES           AGE   VERSION
# master1   Ready    control-plane   5m    v1.29.x
# worker1   Ready    <none>          4m    v1.29.x
# worker2   Ready    <none>          4m    v1.29.x
```

## ğŸ—‚ï¸ Structure du projet

```
.
â”œâ”€â”€ ansible.cfg
â”œâ”€â”€ inventory.ini
â”œâ”€â”€ site.yml
â”œâ”€â”€ uninstall.yml
â”œâ”€â”€ ssh_config
â”œâ”€â”€ group_vars/
â”‚   â””â”€â”€ all.yml
â””â”€â”€ roles/
    â”œâ”€â”€ common/
    â”‚   â””â”€â”€ tasks/main.yml
    â”œâ”€â”€ docker/
    â”‚   â”œâ”€â”€ handlers/main.yml
    â”‚   â””â”€â”€ tasks/main.yml
    â”œâ”€â”€ kubernetes/
    â”‚   â””â”€â”€ tasks/main.yml
    â”œâ”€â”€ master/
    â”‚   â”œâ”€â”€ tasks/main.yml
    â”‚   â””â”€â”€ templates/kubeadm-config.yaml.j2
    â””â”€â”€ worker/
        â””â”€â”€ tasks/main.yml
```
