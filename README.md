# Kubernetes Cluster Automation with Ansible

DÃ©ploiement automatisÃ© d'un cluster Kubernetes (1 Master + Workers) avec Ansible.

## ğŸš€ Ce qui est installÃ©

- **Docker** avec configuration optimisÃ©e (cgroupdriver systemd)
- **cri-dockerd** v0.3.15 (tÃ©lÃ©chargÃ© depuis GitHub, configurÃ© avec `--network-plugin=cni`)
- **CNI plugins** v1.5.0
- **kubeadm, kubelet, kubectl** v1.29
- **Calico** network CNI (CIDR des pods : `192.168.0.0/16`, compatible Calico)
- **Local Path Storage Provisioner** (storage class par dÃ©faut)

## ğŸ“‹ PrÃ©requis

- **Machines Ubuntu 22.04** (ou 20.04)
- **AccÃ¨s SSH** Ã  tous les nÅ“uds
- **Ansible** installÃ© sur la machine de contrÃ´le (version 2.9+)
- **ClÃ© SSH** configurÃ©e (`~/.ssh/id_ed25519`)
- **AccÃ¨s Ã  Internet** depuis chaque VM pendant le dÃ©ploiement (pour tÃ©lÃ©charger cri-dockerd, CNI plugins, Calico et local-path-provisioner)

## ğŸ”§ Configuration

### 1. Configurer les adresses IP statiques sur vos VMs

Configurez les IPs statiques avec Netplan sur chaque VM (fichier `/etc/netplan/00-installer-config.yaml`).

### 2. Configurer SSH

```bash
# GÃ©nÃ©rer une clÃ© SSH
ssh-keygen -t ed25519 -C "k8s-cluster"

# Copier sur toutes les VMs
ssh-copy-id -i ~/.ssh/id_ed25519.pub ubuntu@192.168.1.10
ssh-copy-id -i ~/.ssh/id_ed25519.pub ubuntu@192.168.1.20
ssh-copy-id -i ~/.ssh/id_ed25519.pub ubuntu@192.168.1.30
```

### 3. Configurer sudo sans mot de passe

Sur chaque VM :
```bash
sudo visudo
# Ajouter : ubuntu ALL=(ALL) NOPASSWD:ALL
```

### 4. Modifier l'inventaire

> âš ï¸ **Important** : Les adresses IP `192.168.1.10`, `192.168.1.20` et `192.168.1.30` sont des **exemples**. Remplacez-les par les vraies IPs de vos VMs dans `inventory.ini` **et** dans `ssh_config`.

Ã‰ditez `inventory.ini` avec vos adresses IP :

```ini
[masters]
master1 ansible_host=192.168.1.10

[workers]
worker1 ansible_host=192.168.1.20
worker2 ansible_host=192.168.1.30

[all:vars]
ansible_user=ubuntu
ansible_become=true
ansible_ssh_private_key_file=~/.ssh/id_ed25519
```

### 5. Personnaliser les variables (optionnel)

Ã‰ditez `group_vars/all.yml` pour modifier :
- Version de Kubernetes
- RÃ©seau des pods
- URL Calico

## ğŸš€ DÃ©ploiement

```bash
# Tester la connectivitÃ©
ansible all -i inventory.ini -m ping

# Tester sudo
ansible all -i inventory.ini -m shell -a "sudo whoami"

# VÃ©rifier la syntaxe
ansible-playbook site.yml --syntax-check

# DÃ©ployer le cluster (mode dry-run)
ansible-playbook -i inventory.ini site.yml --check

# DÃ©ployer le cluster
ansible-playbook -i inventory.ini site.yml
```

## âœ… VÃ©rification post-dÃ©ploiement

```bash
# Se connecter au master
ssh ubuntu@192.168.1.10

# VÃ©rifier les nÅ“uds
kubectl get nodes

# Doit afficher :
# NAME      STATUS   ROLES           AGE   VERSION
# master1   Ready    control-plane   5m    v1.29.x
# worker1   Ready    <none>          4m    v1.29.x
# worker2   Ready    <none>          4m    v1.29.x

# VÃ©rifier tous les pods systÃ¨me
kubectl get pods -n kube-system

# VÃ©rifier la storage class par dÃ©faut
kubectl get storageclass

# VÃ©rifier la version de Kubernetes
kubectl version

# VÃ©rifier les infos du cluster
kubectl cluster-info
```

## ğŸ³ Docker Compose

Docker Compose peut Ãªtre installÃ© **sans aucun conflit** avec Kubernetes. C'est un outil indÃ©pendant qui n'interfÃ¨re pas avec kubelet, cri-dockerd ni Calico.

Pour l'installer manuellement sur un nÅ“ud :

```bash
# CrÃ©er le rÃ©pertoire des plugins Docker CLI
sudo mkdir -p /usr/local/lib/docker/cli-plugins

# TÃ©lÃ©charger Docker Compose v2
sudo curl -SL https://github.com/docker/compose/releases/download/v2.27.0/docker-compose-linux-x86_64 \
  -o /usr/local/lib/docker/cli-plugins/docker-compose

# Rendre exÃ©cutable
sudo chmod +x /usr/local/lib/docker/cli-plugins/docker-compose

# VÃ©rifier
docker compose version
```

## ğŸ—‘ï¸ DÃ©sinstallation

Pour dÃ©sinstaller complÃ¨tement le cluster, utilisez le playbook `uninstall.yml` inclus dans le dÃ©pÃ´t :

```bash
# DÃ©sinstaller le cluster
ansible-playbook -i inventory.ini uninstall.yml
```

## ğŸ” DÃ©pannage / Troubleshooting

### ProblÃ¨me 1 â€” Les nÅ“uds restent en Ã©tat `NotReady`

```bash
# VÃ©rifier les pods Calico
kubectl get pods -n kube-system -l k8s-app=calico-node
# VÃ©rifier les logs
kubectl logs -n kube-system -l k8s-app=calico-node
```

### ProblÃ¨me 2 â€” `kubeadm init` Ã©choue avec une erreur CRI

```bash
# VÃ©rifier que cri-dockerd tourne
sudo systemctl status cri-docker.service
sudo systemctl status cri-docker.socket
# Relancer si nÃ©cessaire
sudo systemctl restart cri-docker.socket cri-docker.service
```

### ProblÃ¨me 3 â€” Les workers ne rejoignent pas le cluster

```bash
# VÃ©rifier que kubelet tourne sur le worker
sudo systemctl status kubelet
# VÃ©rifier les logs kubelet
sudo journalctl -u kubelet -n 50
```

### ProblÃ¨me 4 â€” Erreur de connexion SSH Ansible

```bash
# Tester la connectivitÃ©
ansible all -i inventory.ini -m ping
# VÃ©rifier les clÃ©s SSH
ssh -i ~/.ssh/id_ed25519 ubuntu@<IP_VM>
```

### ProblÃ¨me 5 â€” RÃ©initialiser un nÅ“ud manuellement

```bash
sudo kubeadm reset -f
sudo rm -rf /etc/kubernetes /var/lib/kubelet /var/lib/etcd ~/.kube
```

## ğŸ—‚ï¸ Structure du projet

```
.
â”œâ”€â”€ ansible.cfg
â”œâ”€â”€ inventory.ini
â”œâ”€â”€ site.yml
â”œâ”€â”€ uninstall.yml
â”œâ”€â”€ ssh_config
â”œâ”€â”€ group_vars/
â”‚   â””â”€â”€ all.yml
â””â”€â”€ roles/
    â”œâ”€â”€ common/
    â”‚   â””â”€â”€ tasks/main.yml
    â”œâ”€â”€ docker/
    â”‚   â”œâ”€â”€ handlers/main.yml
    â”‚   â””â”€â”€ tasks/main.yml
    â”œâ”€â”€ kubernetes/
    â”‚   â””â”€â”€ tasks/main.yml
    â”œâ”€â”€ master/
    â”‚   â”œâ”€â”€ tasks/main.yml
    â”‚   â””â”€â”€ templates/kubeadm-config.yaml.j2
    â””â”€â”€ worker/
        â””â”€â”€ tasks/main.yml
```
