# Kubernetes Cluster Bootstrap avec Ansible

DÃ©ploiement automatisÃ© d'un cluster Kubernetes (1 Master + Workers) avec Ansible â€” **100% idempotent, modulaire et sans erreurs**.

---

## ğŸš€ Ce qui est installÃ©

| Composant | Version | Description |
|-----------|---------|-------------|
| **Docker** | derniÃ¨re via `docker.io` | Runtime de conteneurs avec cgroupdriver systemd |
| **cri-dockerd** | v0.3.15 | Shim CRI pour Docker, configurÃ© avec `--network-plugin=cni` |
| **CNI plugins** | v1.5.0 | Plugins rÃ©seau pour les conteneurs |
| **kubeadm / kubelet / kubectl** | **1.32** | Outils Kubernetes |
| **Calico** | v3.28.0 | CNI rÃ©seau des pods (CIDR : `10.244.0.0/16`) |
| **Local Path Provisioner** | v0.0.28 | Storage class par dÃ©faut |
| **Docker Compose** | v2.27.0 | Plugin CLI Docker Compose v2 (installÃ© automatiquement) |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Machine de contrÃ´le             â”‚
â”‚              (votre poste / CI)                  â”‚
â”‚         ansible-playbook site.yml                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ SSH
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼           â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ master1 â”‚ â”‚ worker1 â”‚  ...  â”‚ worker2 â”‚
â”‚ control â”‚ â”‚         â”‚       â”‚         â”‚
â”‚  plane  â”‚ â”‚  node   â”‚       â”‚  node   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
192.168.1.10  192.168.1.20    192.168.1.30
              (IPs d'exemple â€” Ã  remplacer)
```

> âš ï¸ **CIDR important** : Le sous-rÃ©seau des VMs est `192.168.1.x`. Calico utilise donc `10.244.0.0/16` pour les pods afin d'Ã©viter tout conflit de routage.

---

## ğŸ“‹ PrÃ©requis

- **Machines Ubuntu 22.04** (ou 20.04) â€” 1 master + N workers
- **AccÃ¨s SSH** Ã  tous les nÅ“uds avec clÃ© ed25519 (`~/.ssh/id_ed25519`)
- **Ansible** â‰¥ 2.9 installÃ© sur la machine de contrÃ´le
- **AccÃ¨s Internet** depuis chaque VM pendant le dÃ©ploiement

---

## ğŸ”§ Configuration pas Ã  pas

### Ã‰tape 1 â€” Configurer les IPs statiques sur vos VMs

Sur chaque VM, configurez une IP statique avec Netplan (`/etc/netplan/00-installer-config.yaml`).

### Ã‰tape 2 â€” Configurer SSH

```bash
# GÃ©nÃ©rer une clÃ© SSH
ssh-keygen -t ed25519 -C "k8s-cluster"

# Copier sur toutes les VMs (remplacez les IPs par les vÃ´tres)
ssh-copy-id -i ~/.ssh/id_ed25519.pub ubuntu@192.168.1.10
ssh-copy-id -i ~/.ssh/id_ed25519.pub ubuntu@192.168.1.20
ssh-copy-id -i ~/.ssh/id_ed25519.pub ubuntu@192.168.1.30
```

### Ã‰tape 3 â€” Configurer sudo sans mot de passe

Sur chaque VM :
```bash
sudo visudo
# Ajouter : ubuntu ALL=(ALL) NOPASSWD:ALL
```

### Ã‰tape 4 â€” âš ï¸ Mettre Ã  jour les IPs (OBLIGATOIRE)

> âš ï¸ **Double mise Ã  jour obligatoire** : Vous devez mettre Ã  jour les adresses IP dans **deux fichiers** :

**`inventory.ini`** :
```ini
[masters]
master1 ansible_host=192.168.1.10   # â† remplacer par votre IP

[workers]
worker1 ansible_host=192.168.1.20   # â† remplacer par votre IP
worker2 ansible_host=192.168.1.30   # â† remplacer par votre IP

[all:vars]
ansible_user=ubuntu
ansible_become=true
ansible_ssh_private_key_file=~/.ssh/id_ed25519
```

**`ssh_config`** :
```
Host master1
    HostName 192.168.1.10   # â† remplacer par votre IP
...
```

### Ã‰tape 5 â€” Personnaliser les variables (optionnel)

Ã‰ditez `group_vars/all.yml` si vous souhaitez modifier les versions ou le CIDR.

---

## ğŸš€ DÃ©ploiement

```bash
# 1. Tester la connectivitÃ©
ansible all -m ping

# 2. VÃ©rifier la syntaxe
ansible-playbook site.yml --syntax-check

# 3. DÃ©ployer le cluster
ansible-playbook site.yml
```

---

## âœ… VÃ©rification post-dÃ©ploiement

```bash
# Se connecter au master
ssh ubuntu@192.168.1.10

# VÃ©rifier les nÅ“uds (tous doivent Ãªtre Ready)
kubectl get nodes
# NAME      STATUS   ROLES           AGE   VERSION
# master1   Ready    control-plane   5m    v1.32.x
# worker1   Ready    <none>          4m    v1.32.x
# worker2   Ready    <none>          4m    v1.32.x

# VÃ©rifier tous les pods systÃ¨me
kubectl get pods -n kube-system

# VÃ©rifier la storage class par dÃ©faut
kubectl get storageclass

# VÃ©rifier la version de Kubernetes
kubectl version

# VÃ©rifier les infos du cluster
kubectl cluster-info
```

---

## ğŸ³ Docker Compose

Docker Compose v2 est **installÃ© automatiquement** par le rÃ´le `docker` sur tous les nÅ“uds. Il n'interfÃ¨re pas avec Kubernetes (kubelet, cri-dockerd, Calico).

```bash
# VÃ©rifier l'installation
docker compose version
# Docker Compose version v2.27.0
```

---

## â™»ï¸ Idempotence

Le playbook `site.yml` est **entiÃ¨rement idempotent** : il peut Ãªtre relancÃ© plusieurs fois sur un cluster dÃ©jÃ  dÃ©ployÃ© sans causer d'erreurs ni de modifications non dÃ©sirÃ©es. Les vÃ©rifications suivantes garantissent l'idempotence :

- **cri-dockerd** : vÃ©rifiÃ© via `stat` avant tÃ©lÃ©chargement
- **Docker Compose** : vÃ©rifiÃ© via `stat` avant tÃ©lÃ©chargement
- **kubeadm init** : vÃ©rifiÃ© via la prÃ©sence de `/etc/kubernetes/admin.conf`
- **Calico / storage provisioner** : appliquÃ©s uniquement si le cluster vient d'Ãªtre initialisÃ©
- **apt-mark hold** : gÃ©rÃ© via `dpkg_selections` (idempotent nativement)

---

## ğŸ—‘ï¸ DÃ©sinstallation

```bash
# DÃ©sinstaller complÃ¨tement le cluster et tous les composants
ansible-playbook -i inventory.ini uninstall.yml
```

Le playbook `uninstall.yml` supprime :
- Les services kubelet, docker, cri-docker
- Les packages (docker.io, kubelet, kubeadm, kubectl)
- Les rÃ©pertoires Kubernetes, Docker, CNI
- Les keyrings et sources APT Kubernetes
- Les configs kube (`~/.kube`) pour root et l'utilisateur
- Les fichiers temporaires (`/tmp/cri-dockerd*`, `/tmp/cni.tgz`, etc.)
- La configuration Docker daemon (`/etc/docker/daemon.json`)

---

## ğŸ” DÃ©pannage

### NÅ“uds en Ã©tat `NotReady`

```bash
kubectl get pods -n kube-system -l k8s-app=calico-node
kubectl logs -n kube-system -l k8s-app=calico-node
```

VÃ©rifiez que le CIDR des pods ne chevauche pas le rÃ©seau des VMs.

### `kubeadm init` Ã©choue avec une erreur CRI

```bash
sudo systemctl status cri-docker.service
sudo systemctl status cri-docker.socket
sudo systemctl restart cri-docker.socket cri-docker.service
```

### Les workers ne rejoignent pas le cluster

```bash
sudo systemctl status kubelet
sudo journalctl -u kubelet -n 50
```

### Erreur de connexion SSH Ansible

```bash
ansible all -m ping
# VÃ©rifier que les IPs dans inventory.ini ET ssh_config correspondent aux vraies IPs
ssh -i ~/.ssh/id_ed25519 ubuntu@<IP_VM>
```

### RÃ©initialiser un nÅ“ud manuellement

```bash
sudo kubeadm reset -f
sudo rm -rf /etc/kubernetes /var/lib/kubelet /var/lib/etcd ~/.kube
```

### Docker Compose non trouvÃ©

```bash
ls -la /usr/local/lib/docker/cli-plugins/docker-compose
docker compose version
```

---

## ğŸ—‚ï¸ Structure du projet

```
.
â”œâ”€â”€ ansible.cfg                          # Configuration Ansible
â”œâ”€â”€ inventory.ini                        # âš ï¸ Ã€ mettre Ã  jour avec vos IPs
â”œâ”€â”€ site.yml                             # Playbook principal
â”œâ”€â”€ uninstall.yml                        # Playbook de dÃ©sinstallation
â”œâ”€â”€ ssh_config                           # âš ï¸ Ã€ mettre Ã  jour avec vos IPs
â”œâ”€â”€ group_vars/
â”‚   â””â”€â”€ all.yml                          # Variables globales
â””â”€â”€ roles/
    â”œâ”€â”€ common/
    â”‚   â””â”€â”€ tasks/main.yml               # Swap, modules kernel, sysctl
    â”œâ”€â”€ docker/
    â”‚   â”œâ”€â”€ handlers/main.yml            # Handler restart Docker
    â”‚   â””â”€â”€ tasks/main.yml               # Docker + cri-dockerd + CNI + Compose
    â”œâ”€â”€ kubernetes/
    â”‚   â”œâ”€â”€ handlers/main.yml            # Handler restart kubelet
    â”‚   â””â”€â”€ tasks/main.yml               # kubeadm/kubelet/kubectl
    â”œâ”€â”€ master/
    â”‚   â”œâ”€â”€ tasks/main.yml               # Init cluster, Calico, storage
    â”‚   â””â”€â”€ templates/kubeadm-config.yaml.j2
    â””â”€â”€ worker/
        â””â”€â”€ tasks/main.yml               # Rejoindre le cluster
```

---

## âš™ï¸ Variables disponibles

| Variable | Valeur par dÃ©faut | Description |
|----------|-------------------|-------------|
| `kubernetes_version` | `"1.32"` | Version de Kubernetes Ã  installer |
| `pod_network_cidr` | `"10.244.0.0/16"` | CIDR rÃ©seau des pods (Calico) |
| `cri_socket` | `"unix:///var/run/cri-dockerd.sock"` | Socket CRI Docker |
| `calico_manifest_url` | URL Calico v3.28.0 | URL du manifest Calico |
| `docker_compose_version` | `"2.27.0"` | Version de Docker Compose v2 |
| `cluster_user` | `"{{ ansible_user }}"` | Utilisateur du cluster |
| `ansible_become` | `true` | Ã‰lÃ©vation de privilÃ¨ges automatique |
